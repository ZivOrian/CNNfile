{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim, autograd as grad\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets \n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code classifies 10 different classes of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.convL1 = nn.Conv2d(in_channels=3,out_channels=64, \n",
    "                                kernel_size=(3,3), stride= (1,1),padding=2)\n",
    "        nn.init.kaiming_normal(self.convL1.weight)\n",
    "        self.convL2 = nn.Conv2d(in_channels=64,out_channels=48, \n",
    "                                kernel_size=(3,3), stride= (1,1),padding=2)\n",
    "        nn.init.kaiming_normal(self.convL2.weight)\n",
    "        self.convL3 = nn.Conv2d(in_channels=48,out_channels=32, \n",
    "                                kernel_size=(2,2), stride= (1,1),padding=1)\n",
    "        nn.init.kaiming_normal(self.convL1.weight)\n",
    "\n",
    "\n",
    "        self.linL4 = nn.Linear(1568, 10, bias=True)\n",
    "        nn.init.xavier_uniform_(self.linL4.weight)\n",
    "        self.pool3 = nn.MaxPool2d((3,3), (3,3))\n",
    "        self.pool2 = nn.MaxPool2d((2,2), (2,2))\n",
    "\n",
    "\n",
    "        self.relu = nn.ReLU()  \n",
    "        self.Batch_norm = nn.BatchNorm1d(num_features=32)\n",
    "        self.Lrelu = nn.LeakyReLU()\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.Softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.Batch_norm(x)\n",
    "        x=self.Lrelu(self.convL1(x))\n",
    "        x=self.pool3(x)\n",
    "        x=self.relu(self.convL2(x))\n",
    "        x=self.pool2(x)\n",
    "        x=self.relu(self.convL3(x))\n",
    "\n",
    "        x=torch.flatten(x)\n",
    "        x=self.Softmax(self.linL4(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.data[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ret_shape(training_set)->str:\n",
    "    ret_string = \"\"\n",
    "    for idx, data in enumerate(training_set):\n",
    "            datas = data[0]\n",
    "            labels = data[1]\n",
    "            ret_string+=\"{}\\n\".format(datas.shape)\n",
    "            ret_string+=\"Labels:{}\\n\".format(labels)\n",
    "            ret_string+=\"Labels shape:{}\\n\".format(len(labels))\n",
    "            ret_string+=\"Labels[0] shape:{}\\n\".format(labels[0].shape)\n",
    "            break\n",
    "    return ret_string\n",
    "\n",
    "\n",
    "def max_index(max_tnsr):\n",
    "    max_tnsr = max_tnsr.tolist()\n",
    "    max_num = 0\n",
    "    for num in max_tnsr:\n",
    "        if max_num < num:\n",
    "            max_num = num\n",
    "    return max_tnsr.index(max_num)\n",
    "\n",
    "def val_func(convNet, cnn_num,valDS, dev): # Selects the best network\n",
    "                                                #   out of n networks\n",
    "    cnns_loss = [0]*cnn_num # represents the 10 cnns\n",
    "    print(\"\\n| Starting validation set run |\\n\")\n",
    "    for val_sample in valDS:\n",
    "        for j in range(cnn_num):\n",
    "            prediction = convNet[j].forward(torch.squeeze(val_sample[0].to(dev)))\n",
    "            loss_func = nn.CrossEntropyLoss()\n",
    "            loss = loss_func(prediction, val_sample[1].to(dev))\n",
    "            cnns_loss[j]+=loss # Adds the loss of the\n",
    "    cnns_loss = [j*-1 for j in cnns_loss]\n",
    "    return convNet[max_index(cnns_loss)]# picks the cnn with the lowest loss\n",
    "\n",
    "def train_one_epoch(cnn, FILE_PATH, train_ds, optimizer, dev, epoch_iter):\n",
    "    iter = 0\n",
    "\n",
    "\n",
    "    # Training loop part\n",
    "    for sample in train_ds:\n",
    "        cnn = cnn.to(dev) # Added due to errors (Should remain in the loop)\n",
    "        targetY = torch.nn.functional.one_hot(\n",
    "            torch.tensor(sample[1]), num_classes=10)\n",
    "        targetY = torch.tensor(targetY, dtype=torch.float32, requires_grad=True)\n",
    "        targetY = torch.squeeze(targetY).to(dev)\n",
    "        \n",
    "        pred = cnn.forward(torch.squeeze(sample[0]).to(dev))\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "        # ---BackPropagation---\n",
    "        loss = loss_func(pred, targetY)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if iter%100==0:\n",
    "            torch.save(cnn.cpu(), FILE_PATH)\n",
    "            grad_tmp = cnn.linL4.weight.grad\n",
    "            print(iter,\"({})\".format(epoch_iter+1), \" | \",grad_tmp.sum()**2/len(grad_tmp))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        iter+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"model.pth\"\n",
    "\n",
    "def train_net(cnn, train_ds, val_ds, dev):\n",
    "    iter = 0\n",
    "    epoch_num = 3 # Determines the epoch number\n",
    "\n",
    "    # Pre-train and validation phase\n",
    "    valtr_num = 2000 # The scale of the minibatch used to train the CNNs that are to be validated\n",
    "    val_train, reg_train = random_split(train_ds, [valtr_num, len(train_ds)-valtr_num])\n",
    "    #train_one_epoch()\n",
    "\n",
    "    # Multiple epoch training phase\n",
    "    optimizer = optim.SGD(params=cnn.parameters(), lr=7.25e-4,\n",
    "                          momentum=0.9,\n",
    "                          weight_decay=1e-4)\n",
    "    for i in range(epoch_num):\n",
    "        train_one_epoch(cnn, FILE_PATH, train_ds, optimizer, dev,i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # Accessing processing unit\n",
    "    if torch.cuda.is_available() : device = \"cuda:0\"\n",
    "    else : device = \"cpu\"\n",
    "    device = torch.device(device)\n",
    "    net = CNN().to(device)\n",
    "\n",
    "    # Loading the datasets\n",
    "    train_data = torchvision.datasets.CIFAR10(\n",
    "        root= \"C:/Users/orian/OneDrive/שולחן העבודה/My Coding Files\"\n",
    "        ,train=True, transform=transforms.Compose([transforms.ToTensor()])\n",
    "        #, target_transform=transforms.Compose([transforms.ToTensor()])\n",
    "    )\n",
    "    train_data = DataLoader(train_data, shuffle=True)\n",
    "\n",
    "    test_data = torchvision.datasets.CIFAR10(\n",
    "        root= \"C:/Users/orian/OneDrive/שולחן העבודה/My Coding Files\"\n",
    "        ,train=False, transform=transforms.Compose([transforms.ToTensor()]), \n",
    "    )\n",
    "    val_data, test_data = random_split(test_data, [1000, len(test_data)-1000])\n",
    "\n",
    "    \n",
    "    train_net(net, train_data, val_data,dev=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = \"model.pth\"\n",
    "model = torch.load(FILE)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "image_lst = [0]*10\n",
    "right_pred = 0\n",
    "for i in range(len(test_data)):\n",
    "    if max_index(model.forward(test_data[i][0]))==test_data[i][1]:\n",
    "        right_pred+=1\n",
    "    image_lst[max_index(model.forward(test_data[i][0]))]+=1\n",
    "print(\"model's accuracy:\",right_pred/100,\"%\")\n",
    "print(image_lst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
